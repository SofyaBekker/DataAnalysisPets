{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ExOkxAzNbYS"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Parser_anecdotes:\n",
        "\n",
        "    def __init__(self, start_url):\n",
        "        self.start_url = start_url\n",
        "\n",
        "    # получение анекдотов и ссылок на след стреницы\n",
        "    def get_text(self, url):\n",
        "        inner_html_code = str(urlopen(url).read(),'utf-8')\n",
        "        inner_soup = BeautifulSoup(inner_html_code, \"html.parser\")\n",
        "        # получаем все анекдоты на странице\n",
        "        anecs = [element.text for element in inner_soup.find_all('div', {\"class\": 'text'})]\n",
        "        # получаем ссылки на следующие страницы\n",
        "        pages_block = inner_soup.find('div', {\"class\": 'pageslist'})\n",
        "        pages_block = pages_block.select('span ~ a')\n",
        "        pages = []\n",
        "        if len(pages_block) != 0:\n",
        "            pages = [self.start_url + tag[\"href\"] for tag in pages_block[:-1]]\n",
        "        return anecs, pages\n",
        "\n",
        "    def write_data(self, file, data):\n",
        "        with open(file, 'a', encoding='utf-8') as f:\n",
        "            for line in data:\n",
        "                f.write(line + '\\n')\n",
        "\n",
        "    # получаем анекдоты из категории и записываем в файл\n",
        "    def get_data_categories(self, file, start_url):\n",
        "        anecs_from_page, pages_url = self.get_text(start_url)\n",
        "        self.write_data(file, anecs_from_page)\n",
        "\n",
        "        url = pages_url\n",
        "\n",
        "        for curr_url in url:\n",
        "            anecs_from_page, pages_url = self.get_text(curr_url)\n",
        "            self.write_data(file, anecs_from_page)\n",
        "            if (len(pages_url) != 0) and (pages_url[-1] not in url):\n",
        "                url.append(pages_url[-1])\n",
        "\n",
        "    # получаем ссылки на категории \"лучшее за *\"\n",
        "    def get_urls(self):\n",
        "        inner_html_code = str(urlopen(self.start_url).read(),'utf-8')\n",
        "        inner_soup = BeautifulSoup(inner_html_code, \"html.parser\")\n",
        "        url_block = inner_soup.find('ul', {\"class\": \"second\"})\n",
        "        urls = [self.start_url + tag[\"href\"] for tag in url_block.select(\"a\")[1:4]]\n",
        "        return urls\n",
        "\n",
        "    # создаем и записываем датасет с анекдотами в file\n",
        "    def create_dataset(self, file):\n",
        "        start_urls = self.get_urls()\n",
        "\n",
        "        for url in start_urls:\n",
        "            self.get_data_categories(file, url)\n",
        "\n",
        "        print(\"Данные получены и записаны в файл \", file)\n"
      ],
      "metadata": {
        "id": "6KyvnAh0N1Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'anecdotes_data.txt'\n",
        "start_url = 'https://www.anekdot.ru'\n",
        "\n",
        "parser = Parser_anecdotes(start_url)\n",
        "parser.create_dataset(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cAxH7DpN4nt",
        "outputId": "c42939ce-9ef8-4086-e7b6-fee449528201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные получены и записаны в файл  anecdotes_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markovify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKTbx97Uad1O",
        "outputId": "79ebc849-b21b-4044-fbb9-eb5b66b83ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading markovify-0.9.4-py3-none-any.whl (19 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markovify\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "euzClqGkOQ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Markov_anecdotes:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.text_model = markovify.Text(text)"
      ],
      "metadata": {
        "id": "c5gtmiooOo0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'anecdotes_data.txt'\n",
        "text = open(file, encoding='utf-8').read()\n",
        "markov_model = Markov_anecdotes(text)\n",
        "print(markov_model.text_model.make_sentence())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyli9sLQOt1h",
        "outputId": "a5a77d64-cf14-4529-91f5-079c99bc0124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "как отдохнули?Общее мнение, что после премьеры фильма «Ирония Судьбы» 1 января эта услуга платная - дополнительные 200 рублей с человека.\n"
          ]
        }
      ]
    }
  ]
}